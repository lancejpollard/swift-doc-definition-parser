{
  "path": "/documentation/accelerate/bnns/activationfunction",
  "type": "Enumeration",
  "name": "BNNS.ActivationFunction",
  "desc": "Constants that describe activation functions.",
  "items": [
    {
      "name": "case abs",
      "desc": "An activation function that returns the absolute value of its input."
    },
    {
      "name": "case celu(alpha: Float)",
      "desc": "An activation function that evaluates the continuously differentiable exponential linear units (CELU) on its input."
    },
    {
      "name": "case clamp(bounds: ClosedRange<Float>)",
      "desc": "An activation function that returns its input clamped to the specified range."
    },
    {
      "name": "case clampedLeakyRectifiedLinear(alpha: Float, beta: Float)",
      "desc": "An activation function that returns its input clamped to beta when that is greater than or equal to zero, otherwise it returns its input multiplied by alpha clamped to beta."
    },
    {
      "name": "case elu(alpha: Float)",
      "desc": "An activation function that evaluates the exponential linear units (ELU) on its input."
    },
    {
      "name": "case geluApproximation(alpha: Float, beta: Float)",
      "desc": "An activation function that evaluates the Gaussian error linear units (GELU) approximation on its input."
    },
    {
      "name": "case geluApproximation2(alpha: Float, beta: Float)",
      "desc": "An activation function that provides a fast evaluation of the Gaussian error linear units (GELU) approximation on its input."
    },
    {
      "name": "case gumbel(alpha: Float, beta: Float)",
      "desc": "An activation function that returns random numbers from the Gumbel distribution."
    },
    {
      "name": "case gumbelMax(alpha: Float, beta: Float)",
      "desc": "An activation function that returns random numbers from the Gumbel distribution."
    },
    {
      "name": "case hardShrink(alpha: Float)",
      "desc": "An activation function that returns zero when the absolute input is less than alpha, otherwise it returns its input."
    },
    {
      "name": "case hardSigmoid(alpha: Float, beta: Float)",
      "desc": "An activation function that returns the hard sigmoid function of its input."
    },
    {
      "name": "case hardSwish(alpha: Float, beta: Float)",
      "desc": "An activation function that returns the hard swish function of its input."
    },
    {
      "name": "case identity",
      "desc": "An activation function that returns its input."
    },
    {
      "name": "case leakyRectifiedLinear(alpha: Float)",
      "desc": "An activation function that returns its input when that is greater than or equal to zero, otherwise it returns its input multiplied by a specified value."
    },
    {
      "name": "case linear(alpha: Float)",
      "desc": "An activation function that returns its input multiplied by a specified value."
    },
    {
      "name": "case linearWithBias(alpha: Float, beta: Float)",
      "desc": "An activation function that returns its input multiplied by a scale and added to a bias."
    },
    {
      "name": "case logSigmoid",
      "desc": "An activation function that returns the logarithm of the sigmoid function of its input."
    },
    {
      "name": "case logSoftmax",
      "desc": "An activation function that returns the logarithm of the softmax function of its input."
    },
    {
      "name": "case rectifiedLinear",
      "desc": "An activation function that returns its input when that is greater than or equal to zero, otherwise it returns zero."
    },
    {
      "name": "case scaledTanh(alpha: Float, beta: Float)",
      "desc": "An activation function that returns the scaled hyperbolic tangent of its input."
    },
    {
      "name": "case selu",
      "desc": "An activation function that evaluates the scaled exponential linear units (SELU) on its input."
    },
    {
      "name": "case sigmoid",
      "desc": "An activation function that returns the sigmoid function of its input."
    },
    {
      "name": "case silu",
      "desc": "An activation function that returns the sigmoid linear unit (SiLU) function of its input."
    },
    {
      "name": "case softShrink(alpha: Float)",
      "desc": "An activation function that returns zero when the absolute input is less than alpha, otherwise it returns its input minus alpha."
    },
    {
      "name": "case softmax",
      "desc": "An activation function that returns the softmax function of its input."
    },
    {
      "name": "case softplus(alpha: Float, beta: Float)",
      "desc": "An activation function that returns the softplus function of its input."
    },
    {
      "name": "case softsign",
      "desc": "An activation function that returns the softsign function of its input."
    },
    {
      "name": "case tanh",
      "desc": "An activation function that returns the hyperbolic tangent of its input."
    },
    {
      "name": "case tanhShrink",
      "desc": "An activation function that returns its input minus the hyperbolic tangent of its input."
    },
    {
      "name": "case threshold(alpha: Float, beta: Float)",
      "desc": "An activation function that returns beta if its input is less than a specified threshold, otherwise it returns its input."
    },
    {
      "name": "var bnnsActivation: BNNSActivation",
      "desc": "The underlying activation function."
    }
  ],
  "declaration": "enum ActivationFunction"
}